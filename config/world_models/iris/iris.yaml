defaults:
  - _self_

training:
  should: True
  learning_rate: 0.0001

  tokenizer:
    batch_num_samples: 256 # 256
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 1
    steps_per_epoch: 1 # 200

  world_model:
    batch_num_samples: 64 # 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    weight_decay: 0.01
    start_after_epochs: 5
    steps_per_epoch: 1 # 200
